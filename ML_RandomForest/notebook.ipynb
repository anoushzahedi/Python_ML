{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a8ca74a-b235-4034-9c1c-3159336a39d5",
   "metadata": {},
   "source": [
    "# Supermarket Loyalty\n",
    "\n",
    "International Essentials is an international supermarket chain.\n",
    "\n",
    "Shoppers at their supermarkets can sign up for a loyalty program that provides rewards each year to customers based on their spending. The more you spend the bigger the rewards. \n",
    "\n",
    "The supermarket would like to be able to predict the likely amount customers in the program will spend, so they can estimate the cost of the rewards. \n",
    "\n",
    "This will help them to predict the likely profit at the end of the year.\n",
    "\n",
    "## Data\n",
    "\n",
    "The dataset contains records of customers for their last full year of the loyalty program.\n",
    "\n",
    "| Column Name | Criteria                                                |\n",
    "|-------------|---------------------------------------------------------|\n",
    "|customer_id | Unique identifier for the customer. </br>Missing values are not possible due to the database structure. |\n",
    "|spend | Continuous. </br>The total spend of the customer in their last full year. This can be any positive value to two decimal places. </br>Missing values should be replaced with 0. |\n",
    "|first_month | Continuous. </br>The amount spent by the customer in their first month of the year. This can be any positive value, rounded to two decimal places. </br>Missing values should be replaced with 0. |\n",
    "| items_in_first_month | Discrete. </br>The number of items purchased in the first month. Any integer value greater than or equal to zero. </br>Missing values should be replaced by 0. |  \n",
    "| region | Nominal. </br>The geographic region that the customer is based in. One of four values Americas, Asia/Pacific, Europe, Middle East/Africa. </br>Missing values should be replaced with \"Unknown\". |\n",
    "| loyalty_years | Oridinal. </br>The number of years the customer has been a part of the loyalty program. One of five ordered categories, '0-1', '1-3', '3-5', '5-10', '10+'. </br>Missing values should be replaced with '0-1'.|\n",
    "| joining_month | Nominal. </br>The month the customer joined the loyalty program. One of 12 values \"Jan\", \"Feb\", \"Mar\", \"Apr\", etc. </br>Missing values should be replaced with \"Unknown\".|\n",
    "| promotion | Nominal. </br>Did the customer join the loyalty program as part of a promotion? Either 'Yes' or 'No'. </br>Missing values should be replaced with 'No'.|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790f8bb8-76fb-44fd-8078-c62909a91b2b",
   "metadata": {},
   "source": [
    "# Task 1\n",
    "\n",
    "Before you fit any models, you will need to make sure the data is clean. \n",
    "\n",
    "The table below shows what the data should look like. \n",
    "\n",
    "Create a cleaned version of the dataframe. \n",
    "\n",
    " - You should start with the data in the file \"loyalty.csv\". \n",
    "\n",
    " - Your output should be a dataframe named `clean_data`. \n",
    "\n",
    " - All column names and values should match the table below.\n",
    "\n",
    "| Column Name | Criteria                                                |\n",
    "|-------------|---------------------------------------------------------|\n",
    "|customer_id | Unique identifier for the customer. </br>Missing values are not possible due to the database structure. |\n",
    "|spend | Continuous. </br>The total spend of the customer in their last full year. This can be any positive value to two decimal places. </br>Missing values should be replaced with 0. |\n",
    "|first_month | Continuous. </br>The amount spent by the customer in their first month of the year. This can be any positive value, rounded to two decimal places. </br>Missing values should be replaced with 0. |\n",
    "| items_in_first_month | Discrete. </br>The number of items purchased in the first month. Any integer value greater than or equal to zero. </br>Missing values should be replaced by 0. |  \n",
    "| region | Nominal. </br>The geographic region that the customer is based in. One of four values Americas, Asia/Pacific, Europe, Middle East/Africa. </br>Missing values should be replaced with \"Unknown\". |\n",
    "| loyalty_years | Oridinal. </br>The number of years the customer has been a part of the loyalty program. One of five ordered categories, '0-1', '1-3', '3-5', '5-10', '10+'. </br>Missing values should be replaced with '0-1'.|\n",
    "| joining_month | Nominal. </br>The month the customer joined the loyalty program. One of 12 values \"Jan\", \"Feb\", \"Mar\", \"Apr\", etc. </br>Missing values should be replaced with \"Unknown\".|\n",
    "| promotion | Nominal. </br>Did the customer join the loyalty program as part of a promotion? Either 'Yes' or 'No'. </br>Missing values should be replaced with 'No'.|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "523dd03c-8591-4cc6-b750-d71da287745f",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 56,
    "lastExecutedAt": 1760898866558,
    "lastExecutedByKernel": "c1a92f1e-0dad-4972-ac33-474c1b544362",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Libs \nimport pandas as pd \nimport numpy as  np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# importing the data\ndf = pd.read_csv('loyalty.csv')\nprint(df.dtypes)\nprint('*'*30)\nprint(df.isna().sum().sort_values(ascending=False))\nprint('*'*30)\n\n## cleaning the database\n#customer_id\ndef data_cleaner(df, test_data=False):\n    clean_data = df.copy()\n    clean_data['customer_id'] = df['customer_id'].astype('category')\n    assert clean_data['customer_id'].nunique() == df.shape[0]\n    if not test_data:\n        # spend\n        clean_data['spend'] = pd.Series([x if x>0 else 0 for x in df['spend']]).round(2)\n    # first_month\n    clean_data['first_month'] = pd.to_numeric(df['first_month'], errors='coerce')\n    clean_data['first_month'] = pd.Series([x if x>0 else 0 for x in clean_data['first_month']]).round(2)\n    # items_in_first_month\n    clean_data['items_in_first_month'] = pd.Series([x if x>0 else 0 for x in df['items_in_first_month']])\n    # region\n    clean_data['region'] = df['region'].astype('category')\n    assert clean_data['region'].nunique() == 4\n    # loyalty_years\n    list_loyalty_years = ['0-1', '1-3', '3-5', '5-10', '10+']\n    assert df['loyalty_years'].isin(list_loyalty_years).all()\n    clean_data['loyalty_years'] = df['loyalty_years'].astype('category').\\\n                                                    cat.reorder_categories(list_loyalty_years, ordered=True)\n    # joining_month\n    clean_data['joining_month'] = df['joining_month'].fillna('Unknown').astype('category')\n    list_month = ['Apr', 'Aug', 'Dec', 'Feb', 'Jan', 'Jun', 'Jul', 'May', 'Mar', 'Nov', 'Oct', 'Sep', 'Unknown']\n    assert clean_data['joining_month'].isin(list_month).all()\n    \n    # promotion\n    clean_data['promotion'] = df['promotion'].str.capitalize().astype('category')\n    list_promotion = ['Yes','No']\n    assert clean_data['promotion'].isin(list_promotion).all()\n    return clean_data\n\n    \n# checking the clean_data\nclean_data = data_cleaner(df)\nclean_data.head()",
    "outputsMetadata": {
     "0": {
      "height": 437,
      "type": "stream"
     },
     "1": {
      "height": 50,
      "tableState": {
       "customFilter": {
        "const": {
         "type": "boolean",
         "valid": true,
         "value": true
        },
        "id": "655e7836-a7a7-485a-9df0-04f050d45104",
        "nodeType": "const"
       }
      },
      "type": "dataFrame"
     },
     "2": {
      "height": 227,
      "type": "stream"
     },
     "3": {
      "height": 500,
      "tableState": {},
      "type": "dataFrame"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "customer_id               int64\n",
      "spend                   float64\n",
      "first_month              object\n",
      "items_in_first_month      int64\n",
      "region                   object\n",
      "loyalty_years            object\n",
      "joining_month            object\n",
      "promotion                object\n",
      "dtype: object\n",
      "******************************\n",
      "joining_month           125\n",
      "customer_id               0\n",
      "spend                     0\n",
      "first_month               0\n",
      "items_in_first_month      0\n",
      "region                    0\n",
      "loyalty_years             0\n",
      "promotion                 0\n",
      "dtype: int64\n",
      "******************************\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>spend</th>\n",
       "      <th>first_month</th>\n",
       "      <th>items_in_first_month</th>\n",
       "      <th>region</th>\n",
       "      <th>loyalty_years</th>\n",
       "      <th>joining_month</th>\n",
       "      <th>promotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>132.68</td>\n",
       "      <td>15.30</td>\n",
       "      <td>5</td>\n",
       "      <td>Asia/Pacific</td>\n",
       "      <td>5-10</td>\n",
       "      <td>Nov</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>106.45</td>\n",
       "      <td>16.20</td>\n",
       "      <td>14</td>\n",
       "      <td>Asia/Pacific</td>\n",
       "      <td>0-1</td>\n",
       "      <td>Feb</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>123.16</td>\n",
       "      <td>25.26</td>\n",
       "      <td>7</td>\n",
       "      <td>Middle East/Africa</td>\n",
       "      <td>10+</td>\n",
       "      <td>Dec</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>130.60</td>\n",
       "      <td>24.74</td>\n",
       "      <td>8</td>\n",
       "      <td>Middle East/Africa</td>\n",
       "      <td>3-5</td>\n",
       "      <td>Apr</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>130.41</td>\n",
       "      <td>25.59</td>\n",
       "      <td>8</td>\n",
       "      <td>Middle East/Africa</td>\n",
       "      <td>3-5</td>\n",
       "      <td>Apr</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  customer_id   spend  first_month  items_in_first_month              region  \\\n",
       "0           1  132.68        15.30                     5        Asia/Pacific   \n",
       "1           2  106.45        16.20                    14        Asia/Pacific   \n",
       "2           3  123.16        25.26                     7  Middle East/Africa   \n",
       "3           4  130.60        24.74                     8  Middle East/Africa   \n",
       "4           5  130.41        25.59                     8  Middle East/Africa   \n",
       "\n",
       "  loyalty_years joining_month promotion  \n",
       "0          5-10           Nov        No  \n",
       "1           0-1           Feb       Yes  \n",
       "2           10+           Dec       Yes  \n",
       "3           3-5           Apr        No  \n",
       "4           3-5           Apr       Yes  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Libs \n",
    "import pandas as pd \n",
    "import numpy as  np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# importing the data\n",
    "df = pd.read_csv('loyalty.csv')\n",
    "print(df.dtypes)\n",
    "print('*'*30)\n",
    "print(df.isna().sum().sort_values(ascending=False))\n",
    "print('*'*30)\n",
    "\n",
    "## cleaning the database\n",
    "#customer_id\n",
    "def data_cleaner(df, test_data=False):\n",
    "    clean_data = df.copy()\n",
    "    clean_data['customer_id'] = df['customer_id'].astype('category')\n",
    "    assert clean_data['customer_id'].nunique() == df.shape[0]\n",
    "    if not test_data:\n",
    "        # spend\n",
    "        clean_data['spend'] = pd.Series([x if x>0 else 0 for x in df['spend']]).round(2)\n",
    "    # first_month\n",
    "    clean_data['first_month'] = pd.to_numeric(df['first_month'], errors='coerce')\n",
    "    clean_data['first_month'] = pd.Series([x if x>0 else 0 for x in clean_data['first_month']]).round(2)\n",
    "    # items_in_first_month\n",
    "    clean_data['items_in_first_month'] = pd.Series([x if x>0 else 0 for x in df['items_in_first_month']])\n",
    "    # region\n",
    "    clean_data['region'] = df['region'].astype('category')\n",
    "    assert clean_data['region'].nunique() == 4\n",
    "    # loyalty_years\n",
    "    list_loyalty_years = ['0-1', '1-3', '3-5', '5-10', '10+']\n",
    "    assert df['loyalty_years'].isin(list_loyalty_years).all()\n",
    "    clean_data['loyalty_years'] = df['loyalty_years'].astype('category').\\\n",
    "                                                    cat.reorder_categories(list_loyalty_years, ordered=True)\n",
    "    # joining_month\n",
    "    clean_data['joining_month'] = df['joining_month'].fillna('Unknown').astype('category')\n",
    "    list_month = ['Apr', 'Aug', 'Dec', 'Feb', 'Jan', 'Jun', 'Jul', 'May', 'Mar', 'Nov', 'Oct', 'Sep', 'Unknown']\n",
    "    assert clean_data['joining_month'].isin(list_month).all()\n",
    "    \n",
    "    # promotion\n",
    "    clean_data['promotion'] = df['promotion'].str.capitalize().astype('category')\n",
    "    list_promotion = ['Yes','No']\n",
    "    assert clean_data['promotion'].isin(list_promotion).all()\n",
    "    return clean_data\n",
    "\n",
    "    \n",
    "# checking the clean_data\n",
    "clean_data = data_cleaner(df)\n",
    "clean_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b695b7-67db-48fb-8b14-e12bb5a9620e",
   "metadata": {},
   "source": [
    "# Task 2 \n",
    "\n",
    "The team at International Essentials have told you that they have always believed that the number of years in the loyalty scheme is the biggest driver of spend. \n",
    "\n",
    "Producing a table showing the difference in the average spend by number of years in the loyalty programme along with the variance to investigate this question for the team.\n",
    "\n",
    " - You should start with the data in the file 'loyalty.csv'.\n",
    "\n",
    " - Your output should be a data frame named `spend_by_years`. \n",
    "\n",
    " - It should include the three columns `loyalty_years`, `avg_spend`, `var_spend`. \n",
    "\n",
    " - Your answers should be rounded to 2 decimal places.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc590298-c483-4253-bef1-a352933cbd5e",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 52,
    "lastExecutedAt": 1760898866610,
    "lastExecutedByKernel": "c1a92f1e-0dad-4972-ac33-474c1b544362",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "spend_by_years = clean_data.groupby('loyalty_years')['spend'].agg(\n    avg_spend=lambda x: x.mean(), \n    var_spend=lambda x: x.std(ddof=0)\n).reset_index().round(2)\nprint(spend_by_years)",
    "outputsMetadata": {
     "0": {
      "height": 143,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  loyalty_years  avg_spend  var_spend\n",
      "0           0-1     110.56       9.30\n",
      "1           1-3     129.31       9.65\n",
      "2           3-5     124.55      11.09\n",
      "3          5-10     135.15      14.10\n",
      "4           10+     117.41      16.72\n"
     ]
    }
   ],
   "source": [
    "spend_by_years = clean_data.groupby('loyalty_years', observed=True)['spend'].agg(\n",
    "    avg_spend=lambda x: x.mean(), \n",
    "    var_spend=lambda x: x.var()\n",
    ").reset_index().round(2)\n",
    "print(spend_by_years)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7113acde-8a74-487a-8983-f0c93003d945",
   "metadata": {},
   "source": [
    "# Task 3\n",
    "\n",
    "Fit a baseline model to predict the spend over the year for each customer.\n",
    "\n",
    " 1. Fit your model using the data contained in “train.csv” </br></br>\n",
    "\n",
    " 2. Use “test.csv” to predict new values based on your model. You must return a dataframe named `base_result`, that includes `customer_id` and `spend`. The `spend` column must be your predicted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "233592eb-4957-47bc-9118-2f9b16630936",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 52,
    "lastExecutedAt": 1760898866662,
    "lastExecutedByKernel": "c1a92f1e-0dad-4972-ac33-474c1b544362",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# reading train and test\ntrain_data = pd.read_csv('train.csv')\ntrain_data = data_cleaner(train_data)\nprint(train_data.shape)\nprint(train_data.head())\nprint('*'*30)\n\ntest_data = pd.read_csv('test.csv')\ntest_data = data_cleaner(test_data, test_data = True)\nprint(test_data.shape)\nprint(test_data.head())\nprint('*'*30)",
    "outputsMetadata": {
     "0": {
      "height": 437,
      "type": "stream"
     },
     "1": {
      "height": 458,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(996, 8)\n",
      "  customer_id   spend  first_month  items_in_first_month              region  \\\n",
      "0           1  135.39        15.44                    10  Middle East/Africa   \n",
      "1           2  129.74        20.12                    12              Europe   \n",
      "2           3  138.61        18.38                     5  Middle East/Africa   \n",
      "3           4  129.94        20.13                     5        Asia/Pacific   \n",
      "4           6   77.56        23.04                    10              Europe   \n",
      "\n",
      "  loyalty_years joining_month promotion  \n",
      "0          5-10           Feb       Yes  \n",
      "1           10+           Jun        No  \n",
      "2          5-10           Oct        No  \n",
      "3           10+           Sep        No  \n",
      "4           1-3           May        No  \n",
      "******************************\n",
      "(250, 7)\n",
      "  customer_id  first_month  items_in_first_month              region  \\\n",
      "0           5        20.43                     8              Europe   \n",
      "1           7        20.90                     7            Americas   \n",
      "2          16        19.99                     7        Asia/Pacific   \n",
      "3          17        23.55                    13  Middle East/Africa   \n",
      "4          19        25.85                    11              Europe   \n",
      "\n",
      "  loyalty_years joining_month promotion  \n",
      "0          5-10           Jul       Yes  \n",
      "1           3-5           Jul        No  \n",
      "2          5-10           Jul       Yes  \n",
      "3           3-5           Aug        No  \n",
      "4           3-5           Apr       Yes  \n",
      "******************************\n"
     ]
    }
   ],
   "source": [
    "# reading train and test\n",
    "train_data = pd.read_csv('train.csv')\n",
    "train_data = data_cleaner(train_data)\n",
    "print(train_data.shape)\n",
    "print(train_data.head())\n",
    "print('*'*30)\n",
    "\n",
    "test_data = pd.read_csv('test.csv')\n",
    "test_data = data_cleaner(test_data, test_data = True)\n",
    "print(test_data.shape)\n",
    "print(test_data.head())\n",
    "print('*'*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79a77f11-b09b-4d70-893b-535dfde919b2",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 18190,
    "lastExecutedAt": 1760898884852,
    "lastExecutedByKernel": "c1a92f1e-0dad-4972-ac33-474c1b544362",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Libs\nfrom sklearn.model_selection import KFold, train_test_split, RandomizedSearchCV\nfrom sklearn.metrics import mean_squared_error as MSE\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.ensemble import RandomForestRegressor\n\n# data\nX = train_data.drop(['spend'], axis=1)\ny = train_data['spend']\n\n# preprocessing\nnumerical_features = X.select_dtypes(include=np.number).columns.tolist()\nnominal_categorical_features = ['region', 'joining_month', 'promotion']\nordinal_categorical_features = ['loyalty_years']\nloyalty_years_order = ['0-1', '1-3', '3-5', '5-10', '10+']\n\nnumerical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='mean')), # Or 'median'\n    ('scaler', StandardScaler())\n])\n\nnominal_categorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')), \n    ('onehot', OneHotEncoder(handle_unknown='ignore', drop='first')) \n])\n\nordinal_categorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('ordinal', OrdinalEncoder(categories=[loyalty_years_order]))\n])\n\n# Combine all transformers using ColumnTransformer\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numerical_transformer, numerical_features),\n        ('nom_cat', nominal_categorical_transformer, nominal_categorical_features),\n        ('ord_cat', ordinal_categorical_transformer, ordinal_categorical_features)\n    ],\n    remainder='passthrough' \n)\n\n# pipeline\nranf_pipe = Pipeline(steps=[\n    ('preprocessor', preprocessor),\n    ('ranf', RandomForestRegressor(random_state=123)) \n])\n\n# model selection\nX_train, X_test, y_train, y_test = train_test_split(\n    X, \n    y, \n    test_size=.2,\n    shuffle=True,\n    random_state=123,\n)\n\n# grid search\nkf = KFold(n_splits=10, random_state=123, shuffle=True)\n\nranf_params = {\n    'ranf__max_depth':np.arange(1,10,1),\n    'ranf__min_samples_leaf':np.arange(.01,.4,.05),\n    'ranf__n_estimators':np.arange(100,210,10)\n}\nranf_cv = RandomizedSearchCV(\n    ranf_pipe,\n    param_distributions=ranf_params,\n    cv=kf,\n    n_iter=10\n)\n\n# results\nranf_result = ranf_cv.fit(X_train, y_train)\ny_pred= ranf_result.predict(X_test)\nprint(f'best score for train_set: {ranf_result.best_score_}')\nprint(f'score for the test set: {ranf_result.score(X_test, y_test)}')\nprint(f'EMSE for the test set: {MSE(y_test, y_pred)**.5}')\n",
    "outputsMetadata": {
     "0": {
      "height": 80,
      "type": "stream"
     },
     "1": {
      "height": 458,
      "type": "stream"
     },
     "3": {
      "height": 458,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best score for train_set: 0.998048579794126\n",
      "score for the test set: 0.9982262926199694\n",
      "RMSE for the test set: 1.098851274674157\n"
     ]
    }
   ],
   "source": [
    "# Libs\n",
    "from sklearn.model_selection import KFold, train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# data\n",
    "# dropiing dependent variable and unique identifier, and dates\n",
    "X = train_data.drop(['spend','customer_id'], axis=1)\n",
    "y = train_data['spend']\n",
    "\n",
    "# preprocessing\n",
    "numerical_features = X.select_dtypes(include=np.number).columns.tolist()\n",
    "nominal_categorical_features = ['region', 'promotion', 'joining_month']\n",
    "ordinal_categorical_features = ['loyalty_years']\n",
    "list_loyalty_years = ['0-1', '1-3', '3-5', '5-10', '10+']\n",
    "\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "nominal_categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')), \n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', drop='first')) \n",
    "])\n",
    "\n",
    "ordinal_categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('ordinal', OrdinalEncoder(categories=[list_loyalty_years]))\n",
    "])\n",
    "\n",
    "# Combine all transformers using ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_features),\n",
    "        ('nom_cat', nominal_categorical_transformer, nominal_categorical_features),\n",
    "        ('ord_cat', ordinal_categorical_transformer, ordinal_categorical_features)\n",
    "    ],\n",
    "    remainder='passthrough' \n",
    ")\n",
    "\n",
    "# pipeline\n",
    "ranf_pipe = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('ranf', RandomForestRegressor(random_state=123)) \n",
    "])\n",
    "\n",
    "# model selection\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, \n",
    "    y, \n",
    "    test_size=.2,\n",
    "    shuffle=True,\n",
    "    random_state=123,\n",
    ")\n",
    "\n",
    "# grid search\n",
    "kf = KFold(n_splits=10, random_state=123, shuffle=True)\n",
    "\n",
    "ranf_params = {\n",
    "    'ranf__max_depth':np.arange(1,10,1),\n",
    "    'ranf__min_samples_leaf':np.arange(.01,.4,.05),\n",
    "    'ranf__n_estimators':np.arange(100,210,10)\n",
    "}\n",
    "ranf_cv = RandomizedSearchCV(\n",
    "    ranf_pipe,\n",
    "    param_distributions=ranf_params,\n",
    "    cv=kf,\n",
    "    n_iter=10,\n",
    "    random_state=123\n",
    ")\n",
    "\n",
    "# results\n",
    "ranf_result = ranf_cv.fit(X_train, y_train)\n",
    "\n",
    "print(f'best score for train_set: {ranf_result.best_score_}')\n",
    "print(f'score for the test set: {ranf_result.score(X_test, y_test)}')\n",
    "print(f'RMSE for the test set: {MSE(y, ranf_result.predict(X))**.5}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "457578db-57cd-4800-9f5f-39eaec219635",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 61,
    "lastExecutedAt": 1760898884913,
    "lastExecutedByKernel": "c1a92f1e-0dad-4972-ac33-474c1b544362",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# task\nbase_result = pd.DataFrame()\nbase_result['customer_id'] = test_data['customer_id']\nbase_result['spend'] = ranf_cv.predict(test_data) \nbase_result.head() ",
    "outputsMetadata": {
     "0": {
      "height": 249,
      "tableState": {
       "customFilter": {
        "const": {
         "type": "boolean",
         "valid": true,
         "value": true
        },
        "id": "655e7836-a7a7-485a-9df0-04f050d45104",
        "nodeType": "const"
       }
      },
      "type": "dataFrame"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "   customer_id  spend_model  spend_solution\n",
      "0            5   141.350648          140.76\n",
      "1            7   149.552646          148.25\n",
      "2           16   141.350648          140.80\n",
      "3           17   150.512630          150.93\n",
      "4           19   150.972166          153.76\n",
      "==================================================\n",
      "RMSE with actual data: 1.233513318812774\n"
     ]
    }
   ],
   "source": [
    "# task\n",
    "base_result = pd.DataFrame()\n",
    "base_result['customer_id'] = test_data['customer_id']\n",
    "base_result['spend'] = ranf_cv.predict(test_data) \n",
    "\n",
    "# getting the actual data provided\n",
    "result = pd.read_csv('.solution_data_files/validation_loyalty.csv')\n",
    "# merging files\n",
    "base_result_solution = base_result.merge(result,  on = 'customer_id', suffixes=('_model', '_solution'))\n",
    "\n",
    "print('='*50)\n",
    "print(base_result_solution.head())\n",
    "print('='*50)\n",
    "\n",
    "# checking the final RMSE\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "\n",
    "print(f\"RMSE with actual data: {MSE(base_result_solution['spend_model'], base_result_solution['spend_solution'])**.5}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44033abf-a603-479e-8663-9a96fefee5a2",
   "metadata": {},
   "source": [
    "# Task 4\n",
    "\n",
    "Fit a comparison model to predict the spend over the year for each customer.\n",
    "\n",
    " 1. Fit your model using the data contained in “train.csv” </br></br>\n",
    "\n",
    " 2. Use “test.csv” to predict new values based on your model. You must return a dataframe named `compare_result`, that includes `customer_id` and `spend`. The `spend` column must be your predicted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "731cfa01-2709-413a-ac19-611e73e37c75",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 9520,
    "lastExecutedAt": 1760898894433,
    "lastExecutedByKernel": "c1a92f1e-0dad-4972-ac33-474c1b544362",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Libs \nfrom sklearn.ensemble import GradientBoostingRegressor\n\n# pipeline\ngradient_pipe = Pipeline(steps=[\n    ('preprocessor', preprocessor),\n    ('gradient', GradientBoostingRegressor(random_state=123)) \n])\n\n# model selection\nX_train, X_test, y_train, y_test = train_test_split(\n    X, \n    y, \n    test_size=.2,\n    shuffle=True,\n    random_state=123,\n)\n\n# grid search\nkf = KFold(n_splits=10, random_state=123, shuffle=True)\n\ngradient_params = {\n    'gradient__max_depth':np.arange(1,10,1),\n    'gradient__min_samples_leaf':np.arange(.01,.4,.05),\n    'gradient__n_estimators':np.arange(100,210,10),\n    'gradient__subsample':np.arange(.1,.9,.1),\n    'gradient__max_features':np.arange(.5,1,.1)\n}\ngradient_cv = RandomizedSearchCV(\n    gradient_pipe,\n    param_distributions=gradient_params,\n    cv=kf,\n    n_iter=10\n)\n\n# results\ngradient_result = gradient_cv.fit(X_train, y_train)\ny_pred= gradient_result.predict(X_test)\nprint(f'best score for train_set: {gradient_result.best_score_}')\nprint(f'score for the test set: {gradient_result.score(X_test, y_test)}')\nprint(f'RMSE for the test set: {MSE(y_test, y_pred)**.5}')",
    "outputsMetadata": {
     "0": {
      "height": 80,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best score for train_set: 0.9980557645979194\n",
      "score for the test set: 0.996940794230792\n",
      "RMSE for the test set: 1.4731731726687796\n"
     ]
    }
   ],
   "source": [
    "# Libs \n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# pipeline\n",
    "gradient_pipe = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('gradient', GradientBoostingRegressor(random_state=123)) \n",
    "])\n",
    "\n",
    "# model selection\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, \n",
    "    y, \n",
    "    test_size=.2,\n",
    "    shuffle=True,\n",
    "    random_state=123,\n",
    ")\n",
    "\n",
    "# grid search\n",
    "kf = KFold(n_splits=10, random_state=123, shuffle=True)\n",
    "\n",
    "gradient_params = {\n",
    "    'gradient__max_depth':np.arange(1,10,1),\n",
    "    'gradient__min_samples_leaf':np.arange(.01,.4,.05),\n",
    "    'gradient__n_estimators':np.arange(100,210,10),\n",
    "    'gradient__subsample':np.arange(.1,.9,.1),\n",
    "    'gradient__max_features':np.arange(.5,1,.1)\n",
    "}\n",
    "gradient_cv = RandomizedSearchCV(\n",
    "    gradient_pipe,\n",
    "    param_distributions=gradient_params,\n",
    "    cv=kf,\n",
    "    n_iter=10,\n",
    "    random_state=123\n",
    ")\n",
    "\n",
    "# results\n",
    "gradient_result = gradient_cv.fit(X_train, y_train)\n",
    "y_pred= gradient_result.predict(X_test)\n",
    "print(f'best score for train_set: {gradient_result.best_score_}')\n",
    "print(f'score for the test set: {gradient_result.score(X_test, y_test)}')\n",
    "print(f'RMSE for the test set: {MSE(y_test, y_pred)**.5}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de5adce0-bc2e-45e7-ad55-19d345ff4a18",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": null,
    "lastExecutedAt": null,
    "lastExecutedByKernel": null,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": null,
    "outputsMetadata": {
     "0": {
      "height": 249,
      "tableState": {
       "customFilter": {
        "const": {
         "type": "boolean",
         "valid": true,
         "value": true
        },
        "id": "655e7836-a7a7-485a-9df0-04f050d45104",
        "nodeType": "const"
       }
      },
      "type": "dataFrame"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "   customer_id  spend_model  spend_solution\n",
      "0            5   141.350648          140.76\n",
      "1            7   149.552646          148.25\n",
      "2           16   141.350648          140.80\n",
      "3           17   150.512630          150.93\n",
      "4           19   150.972166          153.76\n",
      "==================================================\n",
      "RMSE with actual data: 1.389489089011183\n"
     ]
    }
   ],
   "source": [
    "# task\n",
    "compare_result = pd.DataFrame()\n",
    "compare_result['customer_id'] = test_data['customer_id']\n",
    "compare_result['spend'] = gradient_cv.predict(test_data) \n",
    "\n",
    "# getting the actual data provided\n",
    "result = pd.read_csv('.solution_data_files/validation_loyalty.csv')\n",
    "# merging files\n",
    "compare_result_solution = compare_result.merge(result,  on = 'customer_id', suffixes=('_model', '_solution'))\n",
    "\n",
    "print('='*50)\n",
    "print(base_result_solution.head())\n",
    "print('='*50)\n",
    "\n",
    "# checking the final RMSE\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "print(f\"RMSE with actual data: {MSE(compare_result_solution['spend_model'], compare_result_solution['spend_solution'])**.5}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Welcome to DataCamp Workspaces.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
